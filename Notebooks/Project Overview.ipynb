{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Recommender System Project Overview\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "- Steam is a video game digital distribution service. It has a marketplace where users can purchase games as well as several community features such as friends lists, groups, cloud storage, and in-game voice and chat functionality.\n",
    "- In 2019 Steam had over 34,000 games with over 95 million monthly active users and in 2017 (last year available) purchases through Steam totaled $4.3 billion US dollars.\n",
    "- My project is to train a recommender system for Steam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender Systems Summary\n",
    "\n",
    "- A **recommender system** is a model that seeks to predict the preferences a user would give to a collection of items.\n",
    "- Two major classes: content-based filtering and collaborative filtering, although novel hybrid methods are also being developed.\n",
    "- **Content-based filtering** create \"profiles\" of different classes of items and recommending users items from similar profiles to items they have already rated. \n",
    "    - Often implemented via k-means clustering, bayesian classifiers, decision trees, and artificial neural networks\n",
    "    - Pros: easier to train, can make recommendations with relatively little info on the user\n",
    "    - Cons: \"echo chamber\" effect, hard to implement across content types\n",
    "- **Collaborative filtering** identify similar users based upon known common ratings, then recommend items to a target user based on what other similar users have rated highly.\n",
    "    - Often implemented via nearest-neighbor algorithms or by matrix factorization models.\n",
    "    - Pros: does not rely on content features, can learn preferences soley from user interactions with content\n",
    "    - Cons: \"cold start problem\", scalability, sparsity\n",
    "    \n",
    "For this project we will implement a collaborative filtering model based on a matrix factorization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Data Pre-Processing\n",
    "\n",
    "- Source: https://steam.internet.byu.edu/\n",
    "\n",
    "- Collected by researchers at BYU in 2013\n",
    "\n",
    "- ~175GB .sql file that createa a database containing our data and several other tables\n",
    "\n",
    "Since the file too large to load onto a local sql server, so first need to figure out how to extract the data we need. For this I split the file into 170 plain text files of size ~1GB and examined the conents. I skimmed them until I found those that contained the table we want. Here's a portion of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"INSERT INTO `Games_1` VALUES (76561198015117560,41500,NULL,1785,'2013-06-25 08:26:33'),(76561198015117560,400,NULL,5,'2013-06-25 08:26:33'),(76561198015117470,8600,NULL,5,'2013-06-25 08:26:22'),(76561198015117470,8660,NULL,1,'2013-06-25 08:26:22'),(76561198015117430,41500,NULL,1708,'2013-06-25 08:26:18'),(76561198015117430,12210,NULL,969,'2013-06-25 08:26:18'),(76561198015117430,550,NULL,6696,'2013-06-25 08:26:18'),(76561198015117430,223530,NULL,NULL,'2013-06-25 08:26:18'),(76561198015117430,3590,NULL,7385,'2013-06-25 08:26:18'),(76561198015117430,400,NULL,494,'2013-06-25 08:26:18'),(76561198015117430,40930,NULL,61,'2013-06-25 08:26:18'),(76561198015117430,6120,NULL,227,'2013-06-25 08:26:18'),(76561198015117430,340,NULL,NULL,'2013-06-25 08:26:18'),(76561198015117430,380,NULL,NULL,'2013-06-25 08:26:18'),(76561198015117430,420,NULL,NULL,'2013-06-25 08:26:18'),(76561198015117430,440,NULL,312,'2013-06-25 08:26:18'),(76561198015117430,220,NULL,99,'2013-06-25 08:26:18'),(76561198015117430,62\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Volumes/Samsung_T5/Data/Games_Txt/seg_a.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the documentation on the source website, I see that the Games_1 table that we want is organized as follows:\n",
    "\n",
    "- steamid: The steam ID of the user in question\n",
    "- appid: The ID of a given app in the user's library\n",
    "- playtime_2weeks: The total time the user has run this app in the two-week period leading up to when this data was requested from the API. Values are given in minutes.\n",
    "- playtime_forever: The total time the user has run this app since adding it to their library. Values are given in minutes.\n",
    "- dateretrieved: Timestamp of the time when this game data was requested from the API\n",
    "\n",
    "So we can pull the data directly from the .txt files, which are much more manageable. For this project, I only care whether the user bought a game or not, so I just need to store Steam_id and App_id pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(path):\n",
    "    \"\"\"\n",
    "    Input: path to .txt file containing the data\n",
    "    Output: list of lists, each row containing a data point of the form: [Steam Id, App Id, 1]\n",
    "    \"\"\"\n",
    "    # Read File\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Delete SQL commands\n",
    "    text = text.replace(\"INSERT INTO `Games_1` VALUES \", '')\n",
    "\n",
    "    # Split by \"(\"\n",
    "    b = \"),\"\n",
    "    lines = [w + ')' for w in text.split(b) if w]\n",
    "\n",
    "    # Split each line into it's elements\n",
    "    lines = [l.replace(\"(\", \"\").split(\",\") for l in lines]\n",
    "\n",
    "    # Only keep elements we want: steam_id, app_id, and a 1 which will make some next steps easier\n",
    "    lines = np.array([ [np.int64(l[0]), np.int64(l[1]), 1] for l in lines])\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76561198015117560,             41500,                 1],\n",
       "       [76561198015117560,               400,                 1],\n",
       "       [76561198015117470,              8600,                 1],\n",
       "       [76561198015117470,              8660,                 1],\n",
       "       [76561198015117430,             41500,                 1],\n",
       "       [76561198015117430,             12210,                 1],\n",
       "       [76561198015117430,               550,                 1],\n",
       "       [76561198015117430,            223530,                 1],\n",
       "       [76561198015117430,              3590,                 1],\n",
       "       [76561198015117430,               400,                 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = clean_text(path)\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding our model, it is better to think of this data as entries in a (number of users) $\\times$ (number of games) ratings martix $R$ where $R_{ij}$ is 1 if user $i$ would buy game $j$ and is zero otherwise. We can do this by pivoting the data above. In addition, I drop any users who have purchased <10 games since they are hard to make predictions for and because we already have more than enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_sparse(data):\n",
    "    \"\"\"\n",
    "    Inputs: list of data shaped like output of clean_text\n",
    "    Output: pivoted dataframe, drops users with <10 games rated\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data, columns=['steam_id', 'app_id', 'interact'])\n",
    "\n",
    "    # Drop users that interact with < 10 games\n",
    "\n",
    "    grouped = df.groupby('steam_id').aggregate(np.count_nonzero)\n",
    "    keep_ids = grouped[grouped['app_id'] >= 10].index\n",
    "    df = df[df['steam_id'].isin(keep_ids)]\n",
    "\n",
    "    # Create Pivot table\n",
    "\n",
    "    pivot = pd.pivot_table(data= df, values='interact', index='steam_id', columns='app_id')\n",
    "\n",
    "    pivot = pivot.astype(pd.SparseDtype(\"float\", np.nan))\n",
    "\n",
    "    return df, pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127313, 2296)\n",
      "Percent Non-Nan:  1.7427787988072196 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>app_id</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>100</th>\n",
       "      <th>130</th>\n",
       "      <th>...</th>\n",
       "      <th>238130</th>\n",
       "      <th>238170</th>\n",
       "      <th>238210</th>\n",
       "      <th>238530</th>\n",
       "      <th>238630</th>\n",
       "      <th>238710</th>\n",
       "      <th>238890</th>\n",
       "      <th>238930</th>\n",
       "      <th>240600</th>\n",
       "      <th>242110</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steam_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76561197998360190</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76561197998360230</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76561197998360500</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76561197998360540</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76561197998360930</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2296 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "app_id             10      20      30      40      50      60      70      \\\n",
       "steam_id                                                                    \n",
       "76561197998360190     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "76561197998360230     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "76561197998360500     1.0     NaN     1.0     1.0     NaN     1.0     NaN   \n",
       "76561197998360540     1.0     1.0     1.0     1.0     NaN     1.0     NaN   \n",
       "76561197998360930     1.0     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "app_id             80      100     130     ...  238130  238170  238210  \\\n",
       "steam_id                                   ...                           \n",
       "76561197998360190     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "76561197998360230     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "76561197998360500     1.0     1.0     NaN  ...     NaN     NaN     NaN   \n",
       "76561197998360540     1.0     1.0     NaN  ...     NaN     NaN     NaN   \n",
       "76561197998360930     1.0     1.0     NaN  ...     NaN     NaN     NaN   \n",
       "\n",
       "app_id             238530  238630  238710  238890  238930  240600  242110  \n",
       "steam_id                                                                   \n",
       "76561197998360190     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "76561197998360230     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "76561197998360500     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "76561197998360540     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "76561197998360930     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 2296 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sparse_pivot = data_to_sparse(lines)\n",
    "\n",
    "print(sparse_pivot.shape)\n",
    "print('Percent Non-Nan: ',100*np.sum(sparse_pivot.count())/(sparse_pivot.shape[0]*sparse_pivot.shape[1]),'%')\n",
    "sparse_pivot.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latent Factors/ Matrix Factorization Model\n",
    "\n",
    "Let $R$ be an $n\\times m$ ratings matrix, so that $R_{ij}$ denotes the rating user $i$ gave to item $j$. In our case $R$ is 0-1 matrix since our \"ratings\" are binary purchased or not purchased. We may assume that $R$ is large (hundreds of thousands or millions or users and items) and sparse (majority of entries are missing or 0). \n",
    "\n",
    "**Assumption**: every item has a set of attributes, called latent factors, which are shared across all items. Items differ in the degree to which they express these attributes. Additionally, every user has preferences, how much they like or dislike a particular attribute.\n",
    "\n",
    "**Idea**: let $v_j$ be the attribute vector of item $j$ and let $u_i$ be the preference vector of user $i$. Then \n",
    "$$R_{ij} \\approx u_i^Tv_j$$\n",
    "\n",
    "**Goal**: let $k << n,m$. Then we want to find a $m \\times k$ matrix $V$ and a $n \\times k$ matrix $U$ such that\n",
    "$$R \\approx UV^T $$\n",
    "\n",
    "Note that the rows of $V$ are the item attribute vectors and the rows of $U$ are the user preference vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Choosing a Loss Function\n",
    "\n",
    "###  Mean Squared Error:\n",
    "\n",
    "The obvious choice, in spirit of this class, is to take the mean squared error over all the entries in $R$. Then our loss is:\n",
    "$$ \\ell(U,V) = \\frac{1}{nm} \\sum_{ij} ||R_{ij} - U_i V_j^T||^2. $$\n",
    "This is equivalent to trying the minimize the Frobenius norm between $R$ and $U^TV$. Note that this can be solved by Singular Value Decomposition, where we keep only the $k$ largest singular values.\n",
    "\n",
    "**Issues**: due to sparsity the SVD of the matrix will be near zero, which may lead to poor generalization performance. In particular, it will bad at predicting new/ unseen ratings since it will try to force all unseen entries to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Squared-Error over Observed:\n",
    "\n",
    "To avoid this, we could just take the mean squared error over the observed values. Let $\\Omega$ be the observed set. Then:\n",
    "$$ \\ell(U,V) =\\frac{1}{|\\Omega|} \\sum_{ij \\, \\in \\Omega} ||R_{ij} - U_i V_j^T||^2. $$\n",
    "\n",
    "**Issues**: Note that a matrix of all ones would have zero loss but is obviously not what we want. We need to find someone to limit the number of items the model tries to recommend to each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Matrix Factorization Loss\n",
    "\n",
    "The solution is to take the mean squared error over the observed values and add a ridge regularization term forcing $U,V$ to stay small to combat the issues above. Explicitly, we have:\n",
    "\n",
    "$$\\ell(U,V) = \\frac{1}{|\\Omega|} \\sum_{ij \\, \\in \\Omega} ||R_{ij} - U_i V_j^T||^2 + \\lambda \\bigg( \\sum_{i} ||U_i||^2 +  \\sum_j ||V_j||^2 \\bigg)$$\n",
    "\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. In general, increase $\\lambda$ if your data is more sparse as this pushes more non-observed values to zero. Note that if $\\lambda =0$ then this is exactly the same as taking the mean squared error over just the observed entries as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Minimizing the Loss Function: Alternating Least Squares\n",
    " \n",
    "There are two common approaches for minimizing the loss: Stochastic Gradient Descent (SGD) and Alternating Least Squares (ALS). SGD is very flexible and is a popular choice for minimizing many loss functions across many applications. However, by exploiting the special form our loss function takes we can actually minimize the loss much faster using the ALS method which we explain below.\n",
    "\n",
    "We begin by initializing $U$ and $V$ randomly. Now treat $V$ as if it fixed so we are only optimizing over $U$. Then we can rewrite our loss as:\n",
    "\n",
    "$$\\ell(U) \n",
    "= \\frac{1}{|\\Omega|} \\sum_{ij \\, \\in \\Omega} ||R_{ij} - U_i V_j^T||^2 + \\lambda \\bigg( \\sum_{i} ||U_i||^2 +  \\sum_j ||V_j||^2 \\bigg)\n",
    "= \\bigg( \\sum_i \\big(\\frac{1}{|\\Omega|} \\sum_{j: ij \\, \\in \\Omega} ||R_{j} - U_i V_j^T||^2 \\big) + \\lambda||U_i||^2 \\bigg) +\\lambda \\sum_j ||V_j||^2.\n",
    "$$\n",
    "\n",
    "Note that each term in the sum is dependent on a single row of $U$, so we can minimize the loss by minimizing each  term individually. But each term is just an ordinary least squares problem with ridge regularization, which we can solve exactly in polynomial time! Thus we solve these problems for each row of $U$ and update $U$ accordingly.\n",
    "\n",
    "After updating we alernate. Now we treat $U$ as fixed and repeat the process for $V$. Since at each step we are *guaranteed* to decrease the loss, which is not true for SGD, this process is guaranteed to converge and often does so much faster than SGD, usually using only 5-10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "Please continue onto the \"Data Exploration\" notebook see some exploratory data analysis of our dataset, or go straight to the \"Tuning and Evaluation\" notebook to see how we implement this model using PySpark and how it performs on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
